{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.read_csv import review_data\n",
    "from transformers import BertForSequenceClassification, logging\n",
    "from models.sentiment_classifier import SentimentClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import  test_data_tokenizer, label_interpreter\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "logging.set_verbosity_warning()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1> Load the model </h1>\n",
    "\n",
    "3 models are available for testing.\n",
    "\n",
    "\"Full Azerbaijani\" has been trained on nearly 48k data. (remaining 2k data has been lost due unstable internet connection during translation using google translate API)\n",
    "\n",
    "Other models are trained on 21k data to get the results faster\n",
    "\n",
    "\n",
    "Time spent for translation algorithm: 24 hours\n",
    "\n",
    "21k dataset training: 38 mins/epoch\n",
    "\n",
    "\n",
    "48k dataset training: 1h 18 mins / epoch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model from models/latest_model_aze_full.pth ...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "available_models = {\"Azerbaijani\":\"models/latest_model_aze.pth\",\n",
    "                    \"English\": \"models/latest_model.pth\",\n",
    "                    \"Full Azerbaijani\": \"models/latest_model_aze_full.pth\"}\n",
    "\n",
    "\n",
    "test_data = review_data(\"test\")\n",
    "model = SentimentClassifier(2)\n",
    "\n",
    "\n",
    "print('load model from %s ...' % available_models[\"Full Azerbaijani\"])\n",
    "model.load_state_dict(torch.load(available_models[\"Full Azerbaijani\"])['state_dict'])\n",
    "print('Done!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test single sentence </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "review = \"Bu artikl haqqında fikirlərim yaxşıdır. şiddət yoxdur. Hamıya məsləhət görürəm\"\n",
    "model.eval().cpu()\n",
    "\n",
    "input_ids, attention_masks = test_data_tokenizer(review)\n",
    "outputs = model(input_ids= input_ids.unsqueeze(dim= 0),\n",
    "                            attention_mask = attention_masks.unsqueeze(dim = 0))\n",
    "\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "label_interpreter(preds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Batch Testing with test data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if cuda is working 1 min. if not around 10 mins)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 1961/1961 [01:09<00:00, 28.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  tensor(0.8445, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader_test = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=1\n",
    ")\n",
    "\n",
    "print(\"if cuda is working 1 min. if not around 10 mins)\")\n",
    "tq = tqdm.tqdm(total = len(dataloader_test))\n",
    "tq.set_description('Test')\n",
    "\n",
    "test_iterator = enumerate(dataloader_test)\n",
    "correct_predictions=0\n",
    "model.eval().cuda()\n",
    "\n",
    "for _, batch in test_iterator:\n",
    "    with torch.no_grad():\n",
    "        input_ids, attention_masks, label = batch\n",
    "\n",
    "        outputs = model(input_ids= input_ids.cuda(),\n",
    "                                attention_mask = attention_masks.cuda())\n",
    "                \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += torch.sum(preds == label.cuda())\n",
    "                \n",
    "        tq.update()\n",
    "\n",
    "tq.close()\n",
    "\n",
    "print(\"Accuracy: \", correct_predictions/len(dataloader_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f74f65bb7eaf8fab73e4385f5767264cc423549775116fc297cd951f38e125f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
